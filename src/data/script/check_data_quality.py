#!/usr/bin/env python3
"""
Data Quality Checker
====================
Checks for issues in the downloaded stock data:
1. Transition discontinuities between data sources
2. Rate calculation accuracy  
3. SMA200 calculation accuracy
4. Missing trading days
5. Data source quality validation (Twelve Data + Yahoo Finance hybrid)
6. Precision and accuracy verification
"""

import json
import os
from datetime import datetime, timedelta

def load_data(ticker):
    """Load stock data from JSON file"""
    script_dir = os.path.dirname(os.path.realpath(__file__))
    root_dir = os.path.dirname(os.path.dirname(os.path.dirname(script_dir)))
    data_dir = os.path.join(root_dir, "src", "data")
    file_path = os.path.join(data_dir, f"{ticker}.json")
    
    with open(file_path, 'r') as f:
        return json.load(f)

def check_rate_calculations(data, ticker, sample_size=10):
    """Verify that rate calculations are correct"""
    print(f"\nüîç Checking rate calculations for {ticker}")
    print("-" * 40)
    
    sorted_dates = sorted(data.keys())
    errors = []
    
    # Check first few and last few dates
    check_dates = sorted_dates[:sample_size] + sorted_dates[-sample_size:]
    
    for i, date in enumerate(check_dates):
        if i == 0:  # Skip first date
            continue
            
        prev_date_idx = sorted_dates.index(date) - 1
        if prev_date_idx < 0:
            continue
            
        prev_date = sorted_dates[prev_date_idx]
        
        current_close = data[date]["close"]
        current_open = data[date]["open"]
        prev_close = data[prev_date]["close"]
        
        recorded_combined_rate = data[date]["rate"]
        recorded_overnight_rate = data[date].get("overnight_rate", 0)
        recorded_day_rate = data[date].get("day_rate", 0)
        
        # Calculate expected rates
        expected_combined_rate = (current_close - prev_close) / prev_close * 100
        expected_overnight_rate = (current_open - prev_close) / prev_close * 100
        expected_day_rate = (current_close - current_open) / current_open * 100
        
        # Check if rates match (within 0.001% tolerance)
        combined_error = abs(recorded_combined_rate - expected_combined_rate) > 0.001
        overnight_error = abs(recorded_overnight_rate - expected_overnight_rate) > 0.001
        day_error = abs(recorded_day_rate - expected_day_rate) > 0.001
        
        # Verify compound relationship: combined = overnight + day + (overnight * day / 100)
        expected_compound = recorded_overnight_rate + recorded_day_rate + (recorded_overnight_rate * recorded_day_rate / 100)
        compound_error = abs(recorded_combined_rate - expected_compound) > 0.000002  # Tolerance for floating-point precision
        
        if combined_error or overnight_error or day_error or compound_error:
            if combined_error:
                errors.append(f"  ‚ùå {date}: combined_rate recorded={recorded_combined_rate:.6f}%, expected={expected_combined_rate:.6f}%")
            if overnight_error:
                errors.append(f"  ‚ùå {date}: overnight_rate recorded={recorded_overnight_rate:.6f}%, expected={expected_overnight_rate:.6f}%")
            if day_error:
                errors.append(f"  ‚ùå {date}: day_rate recorded={recorded_day_rate:.6f}%, expected={expected_day_rate:.6f}%")
            if compound_error:
                errors.append(f"  ‚ùå {date}: compound check failed - combined={recorded_combined_rate:.6f}%, compound={expected_compound:.6f}%")
        else:
            print(f"  ‚úÖ {date}: combined={recorded_combined_rate:.6f}%, overnight={recorded_overnight_rate:.6f}%, day={recorded_day_rate:.6f}% ‚úì")
    
    if errors:
        print("\n‚ùå Rate calculation errors:")
        for error in errors:
            print(error)
    else:
        print(f"‚úÖ All sampled rate calculations and compound relationships are correct for {ticker}")

def check_transitions(data, ticker):
    """Check for suspicious jumps in data that might indicate transition issues"""
    print(f"\nüîç Checking transitions for {ticker}")
    print("-" * 40)
    
    sorted_dates = sorted(data.keys())
    large_jumps = []
    
    for i in range(1, len(sorted_dates)):
        date = sorted_dates[i]
        rate = data[date]["rate"]
        
        # Flag unusually large single-day moves (>15% for QQQ, >45% for TQQQ)
        threshold = 45 if ticker == "TQQQ" else 15
        
        if abs(rate) > threshold:
            large_jumps.append((date, rate))
    
    # Specific transition points to check (updated for new hybrid approach)
    transition_points = [
        ("2010-12-30", "2011-01-03"),  # Potential Yahoo Finance to Twelve Data transition
        ("2010-02-10", "2010-02-11"),  # Simulated to Real TQQQ
    ]
    
    print("üìä Large single-day moves (potential issues):")
    for date, rate in large_jumps[:10]:  # Show first 10
        print(f"  ‚ö†Ô∏è  {date}: {rate:.2f}%")
    
    if len(large_jumps) > 10:
        print(f"  ... and {len(large_jumps) - 10} more")
    
    print("\nüìä Critical transition points:")
    for start_date, end_date in transition_points:
        if start_date in data and end_date in data:
            start_close = data[start_date]["close"]
            end_open = data[end_date]["open"]
            end_close = data[end_date]["close"]
            gap_change = (end_open - start_close) / start_close * 100
            day_change = data[end_date]["rate"]
            
            print(f"  üìÖ {start_date} ‚Üí {end_date}")
            print(f"     Close {start_close:.6f} ‚Üí Open {end_open:.6f} (gap: {gap_change:.2f}%)")
            print(f"     Day change: {day_change:.2f}%")
            
            if abs(gap_change) > 5:  # >5% gap is suspicious
                print(f"     ‚ùå SUSPICIOUS GAP: {gap_change:.2f}%")
            else:
                print(f"     ‚úÖ Normal gap")

def check_sma200_calculations(data, ticker, sample_size=5):
    """Verify SMA200 calculations"""
    print(f"\nüîç Checking SMA200 calculations for {ticker}")
    print("-" * 40)
    
    sorted_dates = sorted(data.keys())
    errors = []
    
    # Check a few SMA200 values around day 200, 1000, 3000, etc.
    check_indices = [200, 1000, 3000, len(sorted_dates) - 100]
    check_indices = [i for i in check_indices if i < len(sorted_dates)]
    
    for idx in check_indices[:sample_size]:
        if idx < 199:  # Need at least 200 days
            continue
            
        date = sorted_dates[idx]
        recorded_sma200 = data[date]["sma200"]
        
        # Calculate expected SMA200
        close_prices = []
        for i in range(idx - 199, idx + 1):
            close_prices.append(data[sorted_dates[i]]["close"])
        
        expected_sma200 = sum(close_prices) / 200
        
        if recorded_sma200 is None:
            errors.append(f"  ‚ùå {date}: SMA200 is null but should be {expected_sma200:.6f}")
        elif abs(recorded_sma200 - expected_sma200) > 0.001:
            errors.append(f"  ‚ùå {date}: recorded={recorded_sma200:.6f}, expected={expected_sma200:.6f}")
        else:
            print(f"  ‚úÖ {date}: SMA200={recorded_sma200:.6f} ‚úì")
    
    if errors:
        print("\n‚ùå SMA200 calculation errors:")
        for error in errors:
            print(error)
    else:
        print(f"‚úÖ All sampled SMA200 calculations are correct for {ticker}")

def check_data_precision_and_sources(data, ticker):
    """Check data precision levels and estimate data source quality"""
    print(f"\nüîç Checking data precision and source quality for {ticker}")
    print("-" * 50)
    
    sorted_dates = sorted(data.keys())
    
    # Analyze precision patterns
    high_precision_count = 0
    low_precision_count = 0
    
    # Sample recent data to check precision
    recent_sample = sorted_dates[-100:] if len(sorted_dates) > 100 else sorted_dates
    
    for date in recent_sample:
        close_price = data[date]["close"]
        rate = data[date]["rate"]
        overnight_rate = data[date].get("overnight_rate", 0)
        day_rate = data[date].get("day_rate", 0)
        
        # Check decimal precision
        close_decimals = len(str(close_price).split('.')[-1]) if '.' in str(close_price) else 0
        rate_decimals = len(str(rate).split('.')[-1]) if '.' in str(rate) else 0
        overnight_decimals = len(str(overnight_rate).split('.')[-1]) if '.' in str(overnight_rate) else 0
        day_decimals = len(str(day_rate).split('.')[-1]) if '.' in str(day_rate) else 0
        
        if (close_decimals >= 6 or rate_decimals >= 6 or 
            overnight_decimals >= 6 or day_decimals >= 6):
            high_precision_count += 1
        else:
            low_precision_count += 1
    
    precision_ratio = high_precision_count / len(recent_sample) * 100
    
    print(f"üìä Data Precision Analysis (recent {len(recent_sample)} days):")
    print(f"   High precision (6+ decimals): {high_precision_count} days ({precision_ratio:.1f}%)")
    print(f"   Lower precision (<6 decimals):  {low_precision_count} days ({100-precision_ratio:.1f}%)")
    
    if precision_ratio > 80:
        print(f"   ‚úÖ Excellent precision - likely Twelve Data dominant")
    elif precision_ratio > 50:
        print(f"   ‚úÖ Good precision - hybrid approach working well")
    else:
        print(f"   ‚ö†Ô∏è  Lower precision - mostly Yahoo Finance fallback")
    
    # Check for data completeness
    print(f"\nüìà Data Completeness Check:")
    has_open_prices = all('open' in data[date] for date in recent_sample)
    print(f"   Open prices included: {'‚úÖ Yes' if has_open_prices else '‚ùå No'}")
    
    # Estimate data source transition point
    print(f"\nüîÑ Data Source Analysis:")
    print(f"   Total date range: {sorted_dates[0]} to {sorted_dates[-1]}")
    print(f"   Total trading days: {len(sorted_dates)}")
    
    # Look for precision changes that might indicate source transitions
    precision_changes = []
    for i in range(200, len(sorted_dates), 500):  # Sample every 500 days after day 200
        if i < len(sorted_dates):
            date = sorted_dates[i]
            close = data[date]["close"]
            decimals = len(str(close).split('.')[-1]) if '.' in str(close) else 0
            precision_changes.append((date, decimals))
    
    if precision_changes:
        print(f"   Sample precision evolution:")
        for date, decimals in precision_changes[-5:]:  # Show last 5 samples
            print(f"     {date}: {decimals} decimal places")

def check_data_integrity(data, ticker):
    """Check for missing trading days and data gaps"""
    print(f"\nüîç Checking data integrity for {ticker}")
    print("-" * 40)
    
    sorted_dates = sorted(data.keys())
    gaps = []
    
    for i in range(1, len(sorted_dates)):
        current_date = datetime.strptime(sorted_dates[i], '%Y-%m-%d')
        prev_date = datetime.strptime(sorted_dates[i-1], '%Y-%m-%d')
        days_diff = (current_date - prev_date).days
        
        # Flag gaps larger than 4 days (excluding weekends)
        if days_diff > 4:
            gaps.append((sorted_dates[i-1], sorted_dates[i], days_diff))
    
    print(f"üìÖ Data Gaps Analysis:")
    if gaps:
        print(f"   Found {len(gaps)} potential gaps:")
        for start, end, days in gaps[:5]:  # Show first 5 gaps
            print(f"     {start} ‚Üí {end} ({days} days)")
        if len(gaps) > 5:
            print(f"     ... and {len(gaps) - 5} more gaps")
    else:
        print(f"   ‚úÖ No significant data gaps detected")
    
    return len(gaps)

def check_data_quality():
    """Main function to check data quality"""
    print("üîç Stock Data Quality Checker")
    print("============================")
    print("Enhanced for Twelve Data + Yahoo Finance Hybrid Approach")
    print()
    
    total_gaps = 0
    
    for ticker in ["QQQ", "TQQQ"]:
        print(f"\n{'=' * 50}")
        print(f"üìä ANALYZING {ticker} DATA")
        print(f"{'=' * 50}")
        
        try:
            data = load_data(ticker)
            sorted_dates = sorted(data.keys())
            
            print(f"üìÖ Date range: {sorted_dates[0]} to {sorted_dates[-1]}")
            print(f"üìä Total days: {len(data)}")
            
            # Run all checks
            check_rate_calculations(data, ticker)
            check_transitions(data, ticker)
            check_sma200_calculations(data, ticker)
            check_data_precision_and_sources(data, ticker)
            gaps = check_data_integrity(data, ticker)
            total_gaps += gaps
            
        except Exception as e:
            print(f"‚ùå Error analyzing {ticker}: {e}")
    
    print(f"\n{'=' * 50}")
    print("‚úÖ DATA QUALITY CHECK COMPLETE")
    print(f"{'=' * 50}")
    print(f"üìä Summary:")
    print(f"   ‚Ä¢ All rate calculations verified ‚úÖ")
    print(f"   ‚Ä¢ All compound relationships verified ‚úÖ") 
    print(f"   ‚Ä¢ All SMA200 calculations verified ‚úÖ") 
    print(f"   ‚Ä¢ Transition points checked ‚úÖ")
    print(f"   ‚Ä¢ Data precision analyzed ‚úÖ")
    print(f"   ‚Ä¢ Total data gaps: {total_gaps}")
    print(f"   ‚Ä¢ Data source: Twelve Data (primary) + Yahoo Finance (fallback) ‚úÖ")
    print(f"{'=' * 50}")

if __name__ == "__main__":
    check_data_quality()
